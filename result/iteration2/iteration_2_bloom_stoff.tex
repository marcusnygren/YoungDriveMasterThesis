overall comments (more specific in the scanned versions):

-Sometimes difficult to judge the goal – level of questions. E.g. Some traverse A-D and 1-6, depending on how you look at it.
-To what extent are the questions based on terminology definition / facts /mantra provided in the manual / teaching resources?
-Some of the choices are ”obvious” as to the correct answer.
-Too many questions from an assessment point of view. (If for a student to repeat to learn, then it could be okay).
-One category – discussion question
-Can change B to C by asking “How”?
-Often the longer worded options are the correct option. Thus, consider to make the alternatives more equal in text - length.
-Questions aimed at repeating facts or stance versus questions aimed at thinking ”on one’s own”
-It is difficult to reach the C and D levels in a multiple choice questionnaire.
-If you want to have multiple choice questions for assessment there have to be clear-cut differences in the answer choices, without the answer being obvious. (Those that are not clear-cut could be used for discussions).
-Specify if it is a general question – or procedural / practical.
-Specify in what context, sometimes the question is too general.

-Why 3 choices? (would be good to give support from the literature for this. There is some literature saying 3 choices is “best”).
-Also remember that even with a “guess”, still 33% chance of getting answer correct. – (Hence Henrik’s proposal may have potential, but then number of questions would have to be substantially reduced for an assessment purpose.)
-How did you generate the answer choices?
-Need more clarity in. and between. the answer options (sometimes they overlap or blend).
-Specify the questions more – sometimes put very generally.
-Create more spread over A, B, C and D Bloom categories, and perhaps the same balance? (Maybe something to discuss?)
-What form will the analysis take? Will it be a matter of summing the scores for each respondent and comparing them?
-How were the questions validated in terms of the content in the App? (One way to do this is check if you can write “A statement of a learning objective containing a verb (an action) and an object (usually a noun)” for each designed question (as per that really good coloured block Bloom website). In turn, this may also help you to reduce the number of items).
-The distinction between A and B could be decided based on how much terminology and fact related to the question content is in the teaching material.
-How would scores differ between a person on the Youngdrive program versus a person just taking the test (you have some data about this already, since some of the answers could be considered “common sense”. This is not necessarily a bad thing, but would be important to show how the questionnaire is valid for assessing a person that has been on the program / used the app).
-There may be a potential for answering-fatigue (apropos the too many questions comments).
-Would like to see more C and D categories
-Consider an open question for each topic? “How would you…”, allowing for free text to be inserted.
