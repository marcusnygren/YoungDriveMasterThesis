%\section{Learning development}

\section{How can test questions be developed to support entrepreneurship learning?} % Bloom

  The problem identified with multiple-choice questions, regardless if the recommendations by Nicol \cite{nicol} were taken, is that they first could only measure lower-order learning objectives, see figure \ref{fig:revised-bloom}. While entrepreneurial \textit{knowledge} objectives might be considered A-B 1-2, building entrepreneurial \textit{skills} is definitely related to C-D 3-6.

  When assessing the first question sets according to Bloom's revised taxonomy, some characteristics were shown, which guided future creation of questions. Most notably, to reach C-D 3-6 on Bloom, there were some techniques: intelligent multiple-answers could encourage the coach to \textit{evaluate} instead of using process of elminination or encouraging guessing. Putting the coach in a coach scenario (how to act in X situation?), the coach could be tested on a \textit{procedural} and \textit{metacognitive} level to \textit{apply}, \textit{analyse} and \textit{evaluate} skills, and get feedback.

  Previous research by for example Nicol \cite{nicol} had already shown how multiple-choice can be powerful, e.g. by following the principles in figure \ref{fig:multiple-choice}. The same articles mentions the approach taken with using a confidence-meter similar to "Are you sure?". These recommendations have been utilized.

  Some bad questions have still existed, where coaches did not understand and failed to interpret the question, because too advanced English language was used. This points out the value of testing.

  %What is new in this research, is that 1) the confidence-meter is used for feedback, not only assessment, and 2) multiple-choice questions are formulated in a way that supports higher-learning objectives (see Bloom), like entrepreneurship learning.

  %To achieve 2), Bloom's revised taxonomy was used to identify that the standard multiple-choice format did often practice lower order learning objectives (like Factual/Conceptual or Remember/Understand), which fits for entrepreneurial knowledge, but not for entrepreneurship skills. By constructing questions from real-world situations, multiple-choice questions can test procedural knowledge, and the coach may need to also analyse scenarios. The only limitation, is that the coach can not apply the procedural knowledge in the app, but also in real life. Also, as long as the questions are not constructed by the coach herself, she will not reach the \textit{create} level on the cognitive process dimension.

  %\subsubsection{The benefit of having quizzes for the whole YoungDrive training}

  Regarding testing and improving the quality of questions, the initial plan was that YoungDrive would only produce questions for two YoungDrive training weeks, not all 10. To have questions for all of the weeks have greatly benefited the master thesis, and increased the value of the final product. If not all quizzes would have been developed and tested, this would have been a Future Work.

  From a question assessment, it is shown that all of Bloom's levels can now be reached via the app, but two: \textit{create}, and \textit{apply}. This is because users can not create anything in the app, and because of the multiple answers are shown immediately, they are not encouraged to apply their own thinking to the question, before seeing the alternatives.

  % Kritik
  \subsection{Learning effect}
  To the largest extent, the questions have been praised, in regards to formulation and challenge, which can be seen in figure \ref{fig:learning} and \ref{fig:interactiondesign}.

  The lack of a post-test makes it hard to see if the test questions in the app has a real-world effect. Ideally, the post-test in Uganda would have been to observe coaches having their youth session, and compare their correctness and confidence behaviour when not having used the app. %In Zambia, coaches were observed by Josefina, the teacher, but the results are not comparable since all coaches used the app during the training.

  The lack of data (not least from the pre-quiz), makes it hard to draw reliable conclusions from the data. Regarding the pre-quiz paper submissions, the submissions should have been checked for blanks before handed in. Another mistake was that school level was not always exactly specified, and this means that school level and quiz results might have a stronger correlation that can be shown now.

  Regarding recording test question results, this should have been done manually already in iteration 3. The fact that it was done automatically in the app for iteration 4, made so that more data could be recorded, more reliably, than when the project leaders filled them in by hand whenever a coach raised her hand to say she was finished with a quiz.

  The biggest evidence for a learning effect, is the coaches who had a low score on their first try with a quiz, but after the training could pass the certification test, getting 100\% in 1 try. \todo{Improve this section}
