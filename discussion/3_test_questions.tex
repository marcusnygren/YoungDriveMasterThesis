%\section{Learning development}

\section{How Can Test Questions be Developed to Support Entrepreneurship Learning?} % Bloom

  The problem identified with multiple-choice questions, regardless if the recommendations by Nicol \cite{nicol} were taken, is that they first could only measure lower-order learning objectives, see figure \ref{fig:revised-bloom}. While entrepreneurial \textit{knowledge} objectives might be considered A-B 1-2, building entrepreneurial \textit{skills} is definitely related to C-D 3-6.

  When assessing the first question sets according to Bloom's Revised Taxonomy \citep{krathwohl}, some characteristics were shown, which guided future creation of questions. Most notably, to reach C-D 3-6 on Bloom, there were some techniques: intelligent multiple-answers could encourage the coach to \textit{evaluate} instead of using process of elimination or encouraging guessing. Putting the coach in a coach scenario (how to act in X situation?), the coach could be tested on a \textit{procedural} and \textit{metacognitive} level to \textit{apply}, \textit{analyse} and \textit{evaluate} skills, and get feedback.

  Previous research by for example Nicol \cite{nicol} had already shown how multiple-choice can be powerful, for example by following the principles in figure \ref{fig:multiple-choice}. The same articles mentions the approach taken with using a confidence-meter similar to "Are you sure?". These recommendations have been utilized.

  Some bad questions have still existed, where coaches did not understand and failed to interpret the question, because too advanced English language was used. This points out the value of testing.

  %What is new in this research, is that 1) the confidence-meter is used for feedback, not only assessment, and 2) multiple-choice questions are formulated in a way that supports higher-learning objectives (see Bloom), like entrepreneurship learning.

  %To achieve 2), Bloom's revised taxonomy was used to identify that the standard multiple-choice format did often practice lower order learning objectives (like Factual/Conceptual or Remember/Understand), which fits for entrepreneurial knowledge, but not for entrepreneurship skills. By constructing questions from real-world situations, multiple-choice questions can test procedural knowledge, and the coach may need to also analyse scenarios. The only limitation, is that the coach can not apply the procedural knowledge in the app, but also in real life. Also, as long as the questions are not constructed by the coach herself, she will not reach the \textit{create} level on the cognitive process dimension.

  %\subsubsection{The benefit of having quizzes for the whole YoungDrive training}

  Regarding testing and improving the quality of questions, the initial plan was that YoungDrive would only produce questions for two YoungDrive training weeks, not all 10. To have questions for all of the weeks have greatly benefited the master thesis, and increased the value of the final product. If not all quizzes would have been developed and tested, this would have been a Future Work.

  From a question assessment, it is shown that all of Bloom's levels can now be reached via the app, but two: \textit{create}, and \textit{apply}. This is because users can not create anything in the app, and because of the multiple answers are shown immediately, they are not encouraged to apply their own thinking to the question, before seeing the alternatives.

  \subsection{Constructing Good Questions in Entrepreneurship}

  Consider the entrepreneurship topic question "What is financial literacy?" to Bloom's Revised Taxonomy (\textit{conceptual} and \textit{remember}) \citep{krathwohl}. A learning is that to simulate a procedural environment for the coach, a question can effectively be presented as a scenario: "It turns out that 10 youth have not carried out the business action, what should you do?" (\textit{metacognitive} and \textit{evaluating}).

  However, there are several traps that the person formulating the question and answer alternatives can fall into in the case of multiple-choice, where a good question might be de-amplified because of the answer alternatives \citep{nicol}. Consider the coach being asked to give business advice to a fictional youth named Adam: "Adam wants to start a business that is based on a product. which business should he start?". In this case the coach has before been given questions on what a service and product is (factual remember), what the difference is (factual understand), and been given examples (conceptual analyze). Now, the skills are being put to a procedural test. If the answer alternatives are obvious (or memorized), the learning will be lower than scoring high on Bloom's Revised Taxonomy.

  To construct high-quality answer alternatives, all of the answers must be evaluated and considered. In such cases, multiple-choice versus open-ended questions can actually amplify learning, via \textit{learning by repetition} or \textit{learning by thinking}. In the case of the previous question valid alternatives for the coach to consider might have been: "Start a salon", "Start selling soap", "Start a bricklaying business". The coach must evaluate if each alternative is either a service or a product.

  \subsection{Necessary Improvements to the Multiple-Choice Design}
  It is still hard to score high on the knowledge and cognitive dimension using techniques such as multiple-choice with entrepreneurship and coaching. This is however necessary, if the app should reach the learning objectives of YoungDrive. This demanded additions to the multiple-choice design, and not solely content. Such design ideas was "Are you sure?" and giving individual feedback, both of which encouraged metacognitive thinking. Ideas for future work can be read about in see section \ref{sec:future-work-4}.

  % Kritik
  \subsection{Learning Effect}
  To the largest extent, the questions have been praised, in regards to formulation and challenge, which can be seen in figure \ref{fig:learning} and \ref{fig:interactiondesign}. The lack of a post-test makes it hard to see if the test questions in the app has a real-world effect. Ideally, the post-test in Uganda would have been to observe coaches having their youth session, and compare their correctness and confidence behaviour when not having used the app. %In Zambia, coaches were observed by Josefina, the teacher, but the results are not comparable since all coaches used the app during the training.

  The lack of data (not least from the pre-quiz), makes it hard to draw reliable conclusions from the data. Regarding the pre-quiz paper submissions, the submissions should have been checked for blanks before handed in. Another mistake was that school level was not always exactly specified, and this means that school level and quiz results might have a stronger correlation that can be shown now.

  Regarding recording test question results, this should have been done manually already in iteration 3. The fact that it was done automatically in the app for iteration 4, made so that more data could be recorded, more reliably, than when the project leaders filled them in by hand whenever a coach raised her hand to say she was finished with a quiz. A learning effect is indicated partly by that in some cases coaches who had a low score on their first try with a quiz, after the training could pass the certification test, getting 100\% in 1 try. This pattern needs to be more closely examined with a bigger number of testers.
